---
title: "Forecasting: Principles and Practice"
subtitle: "Chapter 7: Exponential Smoothing"
author: "Darshan Patel"
date: "8/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Question 1:
Consider the `pigs` series - the number of pigs slaughtered in Victoria each month. 

(a) Use the `ses()` function in `R` to find the optimal values of $\alpha$ and $l_0$, and generate forecasts for the next four months.

(b) Compute a $95\%$ prediction interval for the first forecast using $\hat{y} \pm 1.96s$ where $s$ is the standard deviation of the residuals. Compare your interval with the interval produced by `R`.


### Question 2: 
Write your own function to implement simple exponential smoothing. The function should take arguments `y` (the time series), `alpha` (the smoothing parameter $\alpha$) and `level` (the initial level $l_0$). It should return the forecast of the next observation in the series. Does it give us the same forecast as `ses()`?


### Question 3:
Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the `optim()` function to find the optimal values of $\alpha$ and $l_0$. Do you get the same values as the `ses()` function?


### Question 4: 
Combine your previous two functions to produce a function which both finds the optimal values of $\alpha$ and $l_0$, and produces a forecast of the next observation in the series.


### Question 5: 
Data sets `books`` contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days' sales for paperback and hardcover books.

(a) Plot the series and discuss the main features of the data.

(b) Use the `ses()` function to forecast each series and plot the forecasts.

(c) Compute the RMSE values for the training data in each case.


### Question 6: 
(a) Now apply Holt's linear method to the `paperback` and `hardback` series and compute four-day forecasts in each case.

(b) Compare the RMSE measures of Holt's method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt's method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.

(c) Compare the forecasts for the two series using both methods. Which do you think is best?

(d) Calculate a $95\%$ prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using `ses` and `holt`.


### Question 7:
For this exercise use data set `eggs`, the price of a dozen eggs in the United States from $1990$ to $1993$. Experiment with the various options in the `holt()` function to see how much the forecast changes with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.
[Hint: use `h=100` when calling `holt()` so you can clearly see the difference between the various options when plotting the forecasts.] 
Which model gives the best RMSE?


### Question 8: 
Recall your retail time series data (from Exercise $3$ in Section 2.10).

(a) Why is multiplicative seasonality necessary for this series?

(b) Apply Holt-Winters' multiplicative method to the data. Experiment with making the trend damped.

(c) Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?

(d) Check that the residuals from the best method look like white noise.

(e) Now fit the test set RMSE, while training the model to the end of $2010$. Can you beat the seasonal naïve approach from Exercise $8$ in Section 3.7?


### Question 9:
For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does this compare with your best previous forecasts on the test set?


### Question 10:
For this exercise use data set `ukcars`, the quarterly UK passenger vehicle production data from $1977$Q$1$ to $2005$Q$1$.

(a) Plot the data and describe the main features of the series.

(b) Decompose the series using STL and obtain the seasonally adjusted data.

(c) Forecast the next two years of the series using an additive damped trend method applied to the seasonally adjusted data. (This can be done in one step using `stlf()` with arguments `etsmodel="AAN", damped=TRUE`.)

(d) Forecast the next two years of the series using Holt's linear method applied to the seasonally adjusted data (as before but with `damped=FALSE`).

(e) Now use `ets()` to choose a seasonal model for the data.

(f) Compare the RMSE of the ETS model with the RMSE of the models you obtained using STL decompositions. Which gives the better in-sample fits?

(g) Compare the forecasts from the three approaches. Which seems more reasonable?

(h) Check the residuals of your preferred model.


### Question 11:
For this exercise use data set `visitors`, the monthly Australian short-term overseas visitors data, May $1985$ to April $2005$. 

(a) Make a time plot of your data and describe the main features of the series.

(b) Split your data into a training set and a test set comprising the last two years of available data. Forecast the test set using Holt-Winters' multiplicative method.

(c) Why is multiplicative seasonality necessary here?

(d) Forecast the two-year test set using each of the following methods:
  i. an ETS model
  
  ii. an additive ETS model applied to a Box-Cox transformed series
  
  iii. a seasonal naïve method
  
  iv. an STL decomposition applied to the Box-Cos transformed data followed by an ETS model applied to the seasonally adjusted (transformed) data
  
(e) Which method gives the best forecasts? Does it pass the residual test?

(f) Compare the same four methods using time series cross-validation with the `tsCV()` function instead of using a training and test set. Do you come to the same conclusions?


### Question 12: 
The `fets()` function below returns ETS forecasts.
```{r}
fets = function(y, h){
  forecast(ets(y), h=h)
}
```

(a) Apply `tsCV()` for a forecast horizon of $h=4$, for both ETS and seasonal naïve methods to the `qcement` data, (Hint: use the newly created `fets()` and the existing `snaive()` functions as your forecast function arguments.)

(b) Compute the MSE of the resulting $4$-step-ahead errors. (Hint: make sure you remove missing values.) Why are there missing values? Comment on which forecasts are more accurate? Is this what you expected?


### Question 13:
Compare `ets()`, `snaive()` and `stlf()` on the following six time series. For `stlf()`, you might need to use a Box-Cox transformation. Use a test set of three years to determine what gives the best forecasts.
`ausbeer`, `bricksq`, `dole`, `a10`, `h02`, `usmelec`.


### Question 14: 
(a) Use `ets()` on the following series: `bicoal`, `chicken`, `dole`, `usdeaths`, `lynx`, `ibmclose`, `eggs`. Does it always give good forecasts?

(b) Find an example where it does not work well. Can you figure out why?


### Question 15: 
Show that the point forecasts from an ETS(M,A,M) model are the same as those obtained using Hot-Winters' multiplicative method.


### Question 16: 
Show that the forecast variance for an ETS(A,N,N) model is given by
$$ \sigma^2[1 + \alpha^2(h-1)] $$ 


### Question 17:
Write down $95\%$ prediction intervals for an ETS(A,N,N) model as a function of $l_T$, $\alpha$, $h$ and $\sigma$, assuming normally distributed errors. 








#### Source: Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on August 1 2019.

